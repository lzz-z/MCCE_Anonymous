import pickle
import json
import uuid
import pandas as pd
from rdkit import Chem
import selfies as sf
import re
import os
import pickle 

tox21_name_dict = {
    "NR-AR": "Androgen Receptor (AR)",
    "NR-AR-LBD": "Androgen Receptor (AR) Ligand-Binding Domain (LBD)",
    "NR-AhR": "Aryl hydrocarbon Receptor (AhR)",
    "NR-Aromatase": "Aromatase receptor",
    "NR-ER": "Estrogen Receptor (ER)",
    "NR-ER-LBD": "Estrogen Receptor (ER) Ligand-Binding Domain (LBD)",
    "NR-PPAR-gamma": "Peroxisome Proliferator-Activated Receptor Gamma (PPAR-gamma)",
    "SR-ARE": "Antioxidant Responsive Element (ARE)",
    "SR-ATAD5": "ATAD5 (ATPase Family AAA Domain Containing 5) gene",
    "SR-HSE": "Heat Shock Sequence (HSE) elements",
    "SR-MMP": "Mitochondrial Membrane Potential (MMP)",
    "SR-p53": "p53 pathway",
}

def transform_key(key):
    return 'tox21_' + key.lower().replace('-', '_')

trm_tox21_name_dict = {transform_key(key): value for key, value in tox21_name_dict.items()}



def to_json(data_list, data_idx_list, name, task_num="1800", dataset_name='bbbp'):
    data = {}
    data['Contributors'] = ['Qizhi Pei']
    data['Categories'] = ['Classification']
    data['Reasoning'] = []
    data['URL'] = ['https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html']
    data['Instruction_language'] = ['English']
    data['Domains'] = ['Chemistry']
    data['Positive Examples'] = []
    data['Negative Examples'] = []
    data['Instances'] = []

    data['Source'] = ['Predict the property of the given molecule.']

    if dataset_name == 'bbbp':
        general_task_definition = f"Molecule property prediction task (a binary classification task) for the BBBP dataset. "
        dataset_desc = "The blood-brain barrier penetration (BBBP) dataset is designed for the modeling and prediction of barrier permeability. "
        instruction = "If the given molecule can penetrate the blood-brain barrier, "
    elif dataset_name == 'bace':
        general_task_definition = f"Molecule property prediction task (a binary classification task) for the BACE dataset. "
        dataset_desc = "The BACE dataset provides qualitative (binary label) binding results for a set of inhibitors of human beta-secretase 1 (BACE-1). "
        instruction = "If the given molecule can inhibit BACE-1, "
    elif dataset_name == 'clintox_ct_tox':
        general_task_definition = f"Molecule property prediction task (a binary classification task) for the ClinTox dataset. "
        dataset_desc = "The ClinTox dataset compares drugs approved by the FDA and drugs that have failed clinical trials for toxicity reasons. "
        instruction = "If the given molecule is toxic, "
    elif 'tox21' in dataset_name:
        general_task_definition = f"Molecule property prediction task (a binary classification task) for the Tox21 dataset. "
        dataset_desc = "The Tox21 dataset contains qualitative toxicity measurements for 8k compounds on 12 different targets, including nuclear receptors and stress response pathways. "
        if dataset_name == 'tox21_sr_atad5':
            instruction = f"If the given molecule can affect {trm_tox21_name_dict['tox21_sr_atad5']}, "
        elif dataset_name == 'tox21_sr_mmp':
            instruction = f"If the given molecule can change {trm_tox21_name_dict['tox21_sr_mmp']}, "
        else:
            instruction = f"If the given molecule can activate {trm_tox21_name_dict[dataset_name]}, "
    else:
        raise NotImplementedError

    cls_definition = f"indicate via \"Yes\". Otherwise, response via \"No\"."
    data['Definition'] = [general_task_definition + dataset_desc + instruction + cls_definition]
    data['Input_language'] = ['English']
    data['Output_language'] = ['English']
    # Each instance must have a unique id, which should be the task number plus a string generated by uuid.uuid4().hex. E.g., task1356-bb5ff013dc5d49d7a962e85ed1de526b.
    for nrow, row in enumerate(data_list):
        data['Instances'].append({
            'id': f'{task_num}-{uuid.uuid4().hex}',
            'input': "Molecule: " + row,
            'output': [data_idx_list[nrow]]
        })
         
    data['Instance License'] = ['Unknown']

    with open(f'{name}.json', 'w') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def rm_map_number(smiles):
    t = re.sub(':\d*', '', smiles)
    return t

def canonicalize(smiles):
    try:
        smiles = rm_map_number(smiles)
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        else:
            return Chem.MolToSmiles(mol)
    except:
        return None

def smiles_to_selfies(smiles):
    return sf.encoder(smiles, strict=False)

def selfies_to_smiles(selfies):
    return sf.decoder(selfies)

def read_inference_output(output_file, col_name, col_idx):
    df = pd.read_csv(output_file)
    if col_name is None:        
        return df.iloc[:, col_idx].tolist()
    else:
        # return 
        return df[col_name].tolist()

import argparse
parser = argparse.ArgumentParser(description='Process some integers.')
parser.add_argument('--task', type=str, default='bbbp', help='task name')
parser.add_argument('--inference_output', type=str, default='inference_output.csv', help='inference output file')
parser.add_argument('--col_name', type=str, default=None, help='column name in inference output file')
parser.add_argument('--col_idx', type=int, default=None, help='column index in inference output file')  
parser.add_argument('--task_tag', type=str, default=None, help='input_mol')
parser.add_argument('--task_class', type=str, default=None,help='1_pret, 2_pret_dpo, 3_pret_sft, 4_pret_sft_dpo')
# parser.add_argument('--eval_type', type=int, default=None, help='evaluation type: 1: all, 2: response all')

args = parser.parse_args()

# if args.eval_type == 1:
#     args.col_idx = 

# data_pick = pickle.load(open(f'test.instruct.{res}.tsv.response.pkl', 'rb'))

data_pick = read_inference_output(args.inference_output, col_name=args.col_name, col_idx=args.col_idx)
invalid_count = 0
output = []
print(f'data_pick number: {len(data_pick)}')
output_idx = []
for idx, item in enumerate(data_pick):
    # pred = item[1][0].strip('.')
    # pred = pred.replace('<mol>', '').replace('</mol>', '')
    # pred = pred.replace('<m>', '').replace(' ', '')
    pred_smi_can = canonicalize(item)
    if pred_smi_can is None:
        invalid_count += 1
        continue
    pred_sfi = smiles_to_selfies(pred_smi_can)
    pred_sfi = '<bom>' + pred_sfi + '<eom>'
    output.append(pred_sfi)
    output_idx.append(idx)


task_name = args.task
print(f'Total {task_name}:', len(data_pick))
print(f'Invalid {task_name}:', invalid_count)

inference_output_file_name =    os.path.splitext(os.path.basename(args.inference_output))[0]


task_dir=f'tasks/{task_name}/{args.task_class}/{inference_output_file_name}'
task_file = f'{task_dir}/{args.task_tag}'
# task_json = f'{task_file}.json'

os.makedirs(task_dir, exist_ok=True)

to_json(output, output_idx, f'{task_file}', task_num="1", dataset_name=task_name)

split_dir = f'splits/{task_name}/{args.task_class}/{inference_output_file_name}/{args.task_tag}'

os.makedirs(split_dir, exist_ok=True)

string_to_write = f"{task_file}"

for split in ['train', 'validation', 'test']:
    with open(f'{split_dir}/{split}_tasks.txt', 'w', encoding='utf-8') as file:
        file.write(string_to_write)
print('process_pkl_bbbp.py done!')